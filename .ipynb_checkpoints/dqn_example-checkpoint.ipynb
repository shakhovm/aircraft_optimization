{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from aircraft_env import Location\n",
    "from aircraft_env import AircraftEnv\n",
    "from utils.units_converter import feet2meter\n",
    "\n",
    "from itertools import product\n",
    "from agents.dql_agent import DQNAgent\n",
    "import numpy as np\n",
    "with open('ddqn_agent/config.yaml') as f:\n",
    "    templates = yaml.safe_load(f)\n",
    "loc_1 = Location(45.46873715, -73.74257166095532)\n",
    "loc_2 = Location(49.0068908, 2.5710819691019156)\n",
    "loc_1, loc_2\n",
    "env = AircraftEnv(arrival_location=loc_1, destination=loc_2, n_waypoints=9)\n",
    "# agent = DQNAgent(templates, env=env, model_type=Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddqn_agent.ddqnF_agent import DDQNFAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNFAgent(templates, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, experiences_per_sampling, seed, compute_weights):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            experiences_per_sampling (int): number of experiences to sample during a sampling iteration\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences_per_sampling = experiences_per_sampling\n",
    "        \n",
    "        self.alpha = 0.5\n",
    "        self.alpha_decay_rate = 0.99\n",
    "        self.beta = 0.5\n",
    "        self.beta_growth_rate = 1.001\n",
    "        self.seed = random.seed(seed)\n",
    "        self.compute_weights = compute_weights\n",
    "        self.experience_count = 0\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", \n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.data = namedtuple(\"Data\", \n",
    "            field_names=[\"priority\", \"probability\", \"weight\",\"index\"])\n",
    "\n",
    "        indexes = []\n",
    "        datas = []\n",
    "        for i in range(buffer_size):\n",
    "            indexes.append(i)\n",
    "            d = self.data(0,0,0,i)\n",
    "            datas.append(d)\n",
    "        \n",
    "        self.memory = {key: self.experience for key in indexes}\n",
    "        self.memory_data = {key: data for key,data in zip(indexes, datas)}\n",
    "        self.sampled_batches = []\n",
    "        self.current_batch = 0\n",
    "        self.priorities_sum_alpha = 0\n",
    "        self.priorities_max = 1\n",
    "        self.weights_max = 1\n",
    "    \n",
    "    def update_priorities(self, tds, indices):\n",
    "        for td, index in zip(tds, indices):\n",
    "            N = min(self.experience_count, self.buffer_size)\n",
    "\n",
    "            updated_priority = td[0]\n",
    "            if updated_priority > self.priorities_max:\n",
    "                self.priorities_max = updated_priority\n",
    "            \n",
    "            if self.compute_weights:\n",
    "                updated_weight = ((N * updated_priority)**(-self.beta))/self.weights_max\n",
    "                if updated_weight > self.weights_max:\n",
    "                    self.weights_max = updated_weight\n",
    "            else:\n",
    "                updated_weight = 1\n",
    "\n",
    "            old_priority = self.memory_data[index].priority\n",
    "            self.priorities_sum_alpha += updated_priority**self.alpha - old_priority**self.alpha\n",
    "            updated_probability = td[0]**self.alpha / self.priorities_sum_alpha\n",
    "            data = self.data(updated_priority, updated_probability, updated_weight, index) \n",
    "            self.memory_data[index] = data\n",
    "\n",
    "    def update_memory_sampling(self):\n",
    "        \"\"\"Randomly sample X batches of experiences from memory.\"\"\"\n",
    "        # X is the number of steps before updating memory\n",
    "        self.current_batch = 0\n",
    "        values = list(self.memory_data.values())\n",
    "        random_values = random.choices(self.memory_data, \n",
    "                                       [data.probability for data in values], \n",
    "                                       k=self.experiences_per_sampling)\n",
    "        self.sampled_batches = [random_values[i:i + self.batch_size] \n",
    "                                    for i in range(0, len(random_values), self.batch_size)]\n",
    "\n",
    "    def update_parameters(self):\n",
    "        self.alpha *= self.alpha_decay_rate\n",
    "        self.beta *= self.beta_growth_rate\n",
    "        if self.beta > 1:\n",
    "            self.beta = 1\n",
    "        N = min(self.experience_count, self.buffer_size)\n",
    "        self.priorities_sum_alpha = 0\n",
    "        sum_prob_before = 0\n",
    "        for element in self.memory_data.values():\n",
    "            sum_prob_before += element.probability\n",
    "            self.priorities_sum_alpha += element.priority**self.alpha\n",
    "        sum_prob_after = 0\n",
    "        for element in self.memory_data.values():\n",
    "            probability = element.priority**self.alpha / self.priorities_sum_alpha\n",
    "            sum_prob_after += probability\n",
    "            weight = 1\n",
    "            if self.compute_weights:\n",
    "                weight = ((N *  element.probability)**(-self.beta))/self.weights_max\n",
    "            d = self.data(element.priority, probability, weight, element.index)\n",
    "            self.memory_data[element.index] = d\n",
    "        print(\"sum_prob before\", sum_prob_before)\n",
    "        print(\"sum_prob after : \", sum_prob_after)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        self.experience_count += 1\n",
    "        index = self.experience_count % self.buffer_size\n",
    "\n",
    "        if self.experience_count > self.buffer_size:\n",
    "            temp = self.memory_data[index]\n",
    "            self.priorities_sum_alpha -= temp.priority**self.alpha\n",
    "            if temp.priority == self.priorities_max:\n",
    "                self.memory_data[index].priority = 0\n",
    "                self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1)).priority\n",
    "            if self.compute_weights:\n",
    "                if temp.weight == self.weights_max:\n",
    "                    self.memory_data[index].weight = 0\n",
    "                    self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2)).weight\n",
    "\n",
    "        priority = self.priorities_max\n",
    "        weight = self.weights_max\n",
    "        self.priorities_sum_alpha += priority ** self.alpha\n",
    "        probability = priority ** self.alpha / self.priorities_sum_alpha\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory[index] = e\n",
    "        d = self.data(priority, probability, weight, index)\n",
    "        self.memory_data[index] = d\n",
    "            \n",
    "    def sample(self):\n",
    "        sampled_batch = self.sampled_batches[self.current_batch]\n",
    "        self.current_batch += 1\n",
    "        experiences = []\n",
    "        weights = []\n",
    "        indices = []\n",
    "        \n",
    "        for data in sampled_batch:\n",
    "            experiences.append(self.memory.get(data.index))\n",
    "            weights.append(data.weight)\n",
    "            indices.append(data.index)\n",
    "\n",
    "        states = torch.from_numpy(\n",
    "            np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(\n",
    "            np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(\n",
    "            np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(\n",
    "            np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(\n",
    "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones, weights, indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.init_new_episode(agent.env)\n",
    "for i in range(2000):\n",
    "    action = np.random.randint(agent.action_number)\n",
    "    processed_action = agent.preprocess_action(action)\n",
    "    next_observation, reward, done = agent.env.step(processed_action)\n",
    "    next_state = agent.preprocess_state(next_observation)\n",
    "    agent.replay_buffer.push((agent.state, action, reward, next_state, done))\n",
    "    agent.state = next_state.copy()\n",
    "    if done:\n",
    "        agent.init_new_episode(agent.env)\n",
    "#         self.rewards.append(episode_reward)\n",
    "#         episode_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, act, reward, next_state, done = \\\n",
    "    map(lambda x: np.array(x), agent.replay_buffer.sample(agent.minibatch))\n",
    "o_state, o_act, o_reward, o_next_state, o_done = agent.transition_process(state, act, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.plot((pd.Series(agent.all_rewards)).rolling(window=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.env_extra import env_summary\n",
    "df = env_summary(env, agent)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fuel_burn'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.Series(agent.periodic_rewards).rolling(window=10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# pd.Series(agent.losses).rolling(window=1000).\n",
    "plt.plot(pd.Series(agent.losses).rolling(window=100).mean())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
